import torch
import os
from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer

# ‚úÖ **Set Model Paths**
MODEL_DIR = "C:/path_to_your_model"
MODEL_FILE = os.path.join(MODEL_DIR, "consolidated.00.pth")
TOKENIZER_FILE = os.path.join(MODEL_DIR, "tokenizer.model")
CONFIG_FILE = os.path.join(MODEL_DIR, "params.json")

# ‚úÖ **Check File Existence**
for file in [MODEL_FILE, TOKENIZER_FILE, CONFIG_FILE]:
    if not os.path.exists(file):
        raise FileNotFoundError(f"‚ùå Missing: {file}")

# ‚úÖ **Load Config**
config = LlamaConfig.from_json_file(CONFIG_FILE)
print(f"‚úÖ Model Config: {config}")

# ‚úÖ **Load Tokenizer**
tokenizer = LlamaTokenizer.from_pretrained(MODEL_DIR)

# ‚úÖ **Check Weights**
state_dict = torch.load(MODEL_FILE, map_location="cpu")
print(f"‚úÖ Model Weights Loaded: {len(state_dict.keys())} keys")

# ‚úÖ **Check for Missing Keys Before Loading**
model = LlamaForCausalLM(config)
missing, unexpected = model.load_state_dict(state_dict, strict=False)
print(f"üîç Missing Keys: {missing}")  # Should be empty
print(f"üîç Unexpected Keys: {unexpected}")  # Should be empty

# ‚úÖ **Move to GPU if available**
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print("‚úÖ Model Successfully Loaded")